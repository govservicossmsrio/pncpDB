#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import logging
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from io import StringIO
import psycopg2
from psycopg2.extras import execute_batch
from typing import Dict, List, Optional, Tuple
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.exceptions import Timeout, ConnectionError

# ===== IMPORTS GOOGLE SHEETS =====
from google.oauth2 import service_account
import gspread

# =====================================================
# CONFIGURA√á√ÉO
# =====================================================

CONFIG = {
    "COCKROACH_CONNECTION_STRING": os.getenv(
        "COCKROACH_CONNECTION_STRING",
        "postgresql://sgc_admin:<password>@scary-quetzal-18026.j77.aws-us-east-1.cockroachlabs.cloud:26257/defaultdb?sslmode=require"
    ),
    "SPREADSHEET_ID": os.getenv("SPREADSHEET_ID", "18P9l9_g-QE-DWsfRCokY18M5RLZe7mV-CWY1bfw6hlA"),
    "SHEET_NAME_CATALOGO": "idlista_catalogo",
    "PNCP_BASE_URL": "https://dadosabertos.compras.gov.br",
    "ENDPOINTS": {
        "MATERIAL": "modulo-pesquisa-preco/1.1_consultarMaterial_CSV",
        "SERVICO": "modulo-pesquisa-preco/3.1_consultarServico_CSV"
    },
    "PAGE_SIZE": 200,
    "PARALLEL_REQUESTS": 3,
    "STAGGER_DELAY_SECONDS": 1,
    "API_ERROR_RETRY_DELAY": 5,
    "MAX_CONSECUTIVE_API_ERRORS": 6,
    "MAX_ERRORS_PER_CODE": 10,
    "EXECUTION_TIME_LIMIT_HOURS": 1,
    "SCRIPT_VERSION": "v2.1.1",
    
    # ===== MODO TESTE =====
    "MODO_TESTE": False,
    "TESTE_CODIGOS": ["439495"],
}

# =====================================================
# LOGGING
# =====================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =====================================================
# VARI√ÅVEIS GLOBAIS DE CONTROLE
# =====================================================

execution_start_time = None
should_stop = False
db_errors_log = []

# =====================================================
# SCHEMA PRECOS_CATALOGO
# =====================================================

PRECOS_SCHEMA = {
    "idcompraitem": "STRING", 
    "idcompra": "STRING",
    "numeroitemcompra": "STRING",  
    "coditemcatalogo": "STRING",
    "unidadeorgaocodigounidade": "STRING",
    "unidadeorgaonomeunidade": "STRING",
    "unidadeorgaouf": "STRING",
    "descricaodetalhada": "STRING",
    "quantidadehomologada": "STRING",
    "unidademedida": "STRING",
    "valorunitariohomologado": "STRING",
    "percentualdesconto": "STRING",
    "marca": "STRING",
    "nifornecedor": "STRING",
    "nomefornecedor": "STRING",
    "datacompra": "STRING",
}

# =====================================================
# CLASSES DE ERRO PERSONALIZADAS
# =====================================================

class APIError(Exception):
    """Erro relacionado √† API (timeout, bloqueio, rate limit)"""
    pass

class DatabaseError(Exception):
    """Erro relacionado ao banco de dados"""
    pass

class DataValidationError(Exception):
    """Erro de valida√ß√£o de dados (CSV malformado)"""
    pass

# =====================================================
# GOOGLE SHEETS - FUN√á√ïES
# =====================================================

def get_sheets_client():
    """Autentica e retorna cliente do Google Sheets"""
    logger.debug("üîê Iniciando autentica√ß√£o Google Sheets...")
    try:
        creds_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        logger.debug(f"üìÑ Arquivo de credenciais: {creds_path}")
        
        if not creds_path or not os.path.exists(creds_path):
            raise ValueError(f"Arquivo de credenciais n√£o encontrado: {creds_path}")
        
        logger.debug("üîë Carregando credenciais do arquivo...")
        credentials = service_account.Credentials.from_service_account_file(
            creds_path,
            scopes=["https://www.googleapis.com/auth/spreadsheets"]
        )
        
        logger.debug("üîó Autorizando cliente gspread...")
        client = gspread.authorize(credentials)
        logger.debug("‚úÖ Autentica√ß√£o Google Sheets conclu√≠da")
        return client
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao inicializar Google Sheets: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

def initialize_sheets_tab():
    """Cria/valida aba idlista_catalogo com cabe√ßalhos"""
    logger.info("üìä Inicializando aba idlista_catalogo...")
    try:
        gc = get_sheets_client()
        spreadsheet = gc.open_by_key(CONFIG["SPREADSHEET_ID"])
        
        # Tenta obter a aba, se n√£o existir, cria
        try:
            sheet = spreadsheet.worksheet(CONFIG["SHEET_NAME_CATALOGO"])
            logger.info(f"‚úÖ Aba '{CONFIG['SHEET_NAME_CATALOGO']}' j√° existe")
        except gspread.WorksheetNotFound:
            logger.info(f"‚ûï Criando aba '{CONFIG['SHEET_NAME_CATALOGO']}'...")
            sheet = spreadsheet.add_worksheet(
                title=CONFIG["SHEET_NAME_CATALOGO"],
                rows=1000,
                cols=4
            )
        
        # Verifica/adiciona cabe√ßalhos
        values = sheet.get_all_values()
        if not values or values[0] != ["cod_br", "idcompra", "status", "ultima_busca"]:
            logger.info("üìù Configurando cabe√ßalhos da planilha...")
            # CORRE√á√ÉO: Ordem correta dos argumentos (values primeiro)
            sheet.update(values=[["cod_br", "idcompra", "status", "ultima_busca"]], range_name='A1:D1')
            logger.info("‚úÖ Cabe√ßalhos configurados")
        
        return sheet
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao inicializar aba Sheets: {e}")
        # N√£o levanta exce√ß√£o - processamento pode continuar sem Sheets
        return None

def write_catalogo_status(codigo: str, idcompra_list: Optional[List[str]], status: str):
    """
    Escreve/atualiza status na aba idlista_catalogo
    
    Args:
        codigo: C√≥digo do cat√°logo (cod_br)
        idcompra_list: Lista de idcompra encontrados (None se erro antes de obter dados)
        status: Mensagem de status
    """
    logger.debug(f"üìä Preparando escrita no Sheets para c√≥digo {codigo}...")
    
    try:
        # Formata coluna idcompra
        if idcompra_list and len(idcompra_list) > 0:
            sample = idcompra_list[:3]
            idcompra_display = "; ".join(sample)
            if len(idcompra_list) > 3:
                idcompra_display += f" (+{len(idcompra_list) - 3})"
            logger.debug(f"üìã IDs formatados: {idcompra_display}")
        else:
            idcompra_display = "N/A"
            logger.debug("üìã Nenhum ID encontrado, usando 'N/A'")
        
        # Timestamp UTC
        dt = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
        logger.debug(f"üïí Timestamp: {dt}")
        
        # Abre planilha
        logger.debug("üîó Conectando ao Google Sheets...")
        gc = get_sheets_client()
        sheet = gc.open_by_key(CONFIG["SPREADSHEET_ID"]).worksheet(CONFIG["SHEET_NAME_CATALOGO"])
        
        # Busca linha existente
        logger.debug(f"üîç Procurando c√≥digo {codigo} na planilha...")
        values = sheet.get_all_values()
        
        row_index = None
        for i in range(1, len(values)):  # Pula cabe√ßalho
            if values[i][0].strip() == codigo:
                row_index = i + 1  # +1 porque sheet √© 1-indexed
                logger.debug(f"‚úì C√≥digo encontrado na linha {row_index}")
                break
        
        # Atualiza ou insere
        if row_index:
            logger.debug(f"üîÑ Atualizando linha existente {row_index}...")
            sheet.update_cell(row_index, 2, idcompra_display)  # Coluna B
            sheet.update_cell(row_index, 3, status)            # Coluna C
            sheet.update_cell(row_index, 4, dt)                # Coluna D
            logger.info(f"‚úÖ Status atualizado no Sheets para {codigo}")
        else:
            logger.debug(f"‚ûï Inserindo nova linha para c√≥digo {codigo}...")
            sheet.append_row([codigo, idcompra_display, status, dt])
            logger.info(f"‚úÖ Nova linha criada no Sheets para {codigo}")
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao escrever no Sheets para {codigo}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        # N√£o levanta exce√ß√£o - processamento continua

def populate_initial_codes():
    """Popula aba com todos os c√≥digos que ser√£o processados (apenas cod_br)"""
    logger.info("üìù Populando c√≥digos iniciais na planilha...")
    try:
        pending_codes = get_pending_codes()
        
        if not pending_codes:
            logger.info("‚ÑπÔ∏è Nenhum c√≥digo para popular")
            return
        
        gc = get_sheets_client()
        sheet = gc.open_by_key(CONFIG["SPREADSHEET_ID"]).worksheet(CONFIG["SHEET_NAME_CATALOGO"])
        
        # Obter c√≥digos j√° existentes
        values = sheet.get_all_values()
        existing_codes = {row[0].strip() for row in values[1:] if row}  # Pula cabe√ßalho
        
        # Filtrar novos c√≥digos
        new_codes = [(codigo, tipo) for codigo, tipo in pending_codes if codigo not in existing_codes]
        
        if new_codes:
            logger.info(f"‚ûï Adicionando {len(new_codes)} novos c√≥digos √† planilha...")
            rows_to_add = [[codigo, "", "pendente", ""] for codigo, _ in new_codes]
            
            # Adiciona em lotes de 100
            for i in range(0, len(rows_to_add), 100):
                batch = rows_to_add[i:i+100]
                sheet.append_rows(batch)
                logger.debug(f"‚úì Lote {i//100 + 1} adicionado ({len(batch)} c√≥digos)")
            
            logger.info(f"‚úÖ {len(new_codes)} c√≥digos adicionados √† planilha")
        else:
            logger.info("‚úÖ Todos os c√≥digos j√° est√£o na planilha")
            
    except Exception as e:
        logger.error(f"‚ùå Erro ao popular c√≥digos iniciais: {e}")
        # N√£o levanta exce√ß√£o - processamento continua

# =====================================================
# FUN√á√ïES AUXILIARES
# =====================================================

def normalizar_nome_coluna(nome: str) -> str:
    """Converte CamelCase para snake_case"""
    if not isinstance(nome, str):
        return ''
    s = nome.strip()
    s = re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', s)
    s = re.sub(r'[^a-zA-Z0-9_]+', '_', s)
    return s.lower().strip('_')

def convert_brazilian_number_to_decimal(value) -> Optional[str]:
    """
    Converte n√∫mero brasileiro para formato decimal aceito pelo banco
    
    Exemplos:
        "1,00" ‚Üí "1.00"
        "4.668,00" ‚Üí "4668.00"
        "19.760,00" ‚Üí "19760.00"
        "" ‚Üí None
        None ‚Üí None
    """
    if pd.isna(value) or value is None or value == '' or str(value).strip() == '':
        return None
    
    value_str = str(value).strip()
    
    # Trata strings que representam valores nulos
    if value_str.lower() in ['null', 'none', 'nan', 'nat', '<na>']:
        return None
    
    # Remove pontos de milhar e troca v√≠rgula por ponto
    # Formato brasileiro: 1.234.567,89
    # Formato americano: 1234567.89
    value_str = value_str.replace('.', '')  # Remove pontos de milhar
    value_str = value_str.replace(',', '.')  # Troca v√≠rgula por ponto
    
    return value_str

def convert_to_string_safe(value) -> Optional[str]:
    """
    Converte valor para string de forma segura, retornando None para vazios
    
    CORRE√á√ÉO: Trata strings "null", "None", "nan" como None
    """
    if pd.isna(value) or value is None or value == '':
        return None
    
    value_str = str(value).strip()
    
    # Trata strings que representam valores nulos
    if value_str.lower() in ['null', 'none', 'nan', 'nat', '<na>']:
        return None
    
    if value_str == '':
        return None
    
    return value_str

def convert_to_integer_safe(value) -> Optional[int]:
    """
    Converte valor para integer de forma segura, retornando None para inv√°lidos
    
    NOVO: Fun√ß√£o espec√≠fica para campos INTEGER
    """
    if pd.isna(value) or value is None or value == '':
        return None
    
    value_str = str(value).strip()
    
    # Trata strings que representam valores nulos
    if value_str.lower() in ['null', 'none', 'nan', 'nat', '<na>', '']:
        return None
    
    # Remove decimais se for n√∫mero float (ex: "123.0" -> "123")
    if '.' in value_str:
        try:
            float_val = float(value_str)
            if float_val.is_integer():
                value_str = str(int(float_val))
            else:
                # Se tem decimal n√£o-zero, tenta arredondar
                value_str = str(round(float_val))
        except:
            return None
    
    try:
        return int(value_str)
    except (ValueError, TypeError):
        logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel converter '{value_str}' para INTEGER - usando None")
        return None

def convert_to_date_safe(value) -> Optional[str]:
    """
    Converte valor para data de forma segura, retornando None para inv√°lidos
    
    NOVO: Fun√ß√£o espec√≠fica para campos DATE
    """
    if pd.isna(value) or value is None or value == '':
        return None
    
    value_str = str(value).strip()
    
    # Trata strings que representam valores nulos
    if value_str.lower() in ['null', 'none', 'nan', 'nat', '<na>', '']:
        return None
    
    # Tenta converter para data
    try:
        date_obj = pd.to_datetime(value_str, errors='coerce')
        if pd.isna(date_obj):
            return None
        return date_obj.strftime('%Y-%m-%d')
    except:
        return None

def check_execution_time() -> bool:
    """Verifica se o tempo de execu√ß√£o foi excedido"""
    global execution_start_time, should_stop
    
    if should_stop:
        return False
    
    elapsed = datetime.now() - execution_start_time
    limit = timedelta(hours=CONFIG["EXECUTION_TIME_LIMIT_HOURS"])
    
    if elapsed >= limit:
        logger.warning(f"‚è∞ Tempo de execu√ß√£o atingido: {elapsed} >= {limit}")
        should_stop = True
        return False
    
    return True

# =====================================================
# CONEX√ÉO COM COCKROACHDB
# =====================================================

def get_db_connection():
    """Cria conex√£o com CockroachDB"""
    try:
        conn = psycopg2.connect(CONFIG["COCKROACH_CONNECTION_STRING"])
        return conn
    except Exception as e:
        logger.error(f"Erro ao conectar com CockroachDB: {e}")
        raise DatabaseError(f"Falha na conex√£o: {e}")

# =====================================================
# TABELA DE CONTROLE DE EXECU√á√ÉO
# =====================================================

def create_control_table():
    """Cria tabela de controle de execu√ß√£o se n√£o existir"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS precos_catalogo_controle (
                codigo_catalogo STRING PRIMARY KEY,
                tipo STRING NOT NULL,
                tentativas_totais INT DEFAULT 0,
                ultima_tentativa TIMESTAMP,
                ultimo_erro TEXT,
                ultimo_sucesso TIMESTAMP,
                status STRING DEFAULT 'PENDENTE'
            )
        """)
        
        conn.commit()
        cursor.close()
        conn.close()
        logger.info("‚úì Tabela de controle verificada/criada")
        
    except Exception as e:
        logger.error(f"Erro ao criar tabela de controle: {e}")
        raise DatabaseError(f"Falha ao criar tabela de controle: {e}")

def update_control_record(codigo: str, tipo: str, success: bool, error_msg: Optional[str] = None):
    """Atualiza registro de controle de execu√ß√£o"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if success:
            cursor.execute("""
                INSERT INTO precos_catalogo_controle 
                    (codigo_catalogo, tipo, tentativas_totais, ultima_tentativa, ultimo_sucesso, status)
                VALUES (%s, %s, 1, NOW(), NOW(), 'SUCESSO')
                ON CONFLICT (codigo_catalogo)
                DO UPDATE SET
                    tentativas_totais = precos_catalogo_controle.tentativas_totais + 1,
                    ultima_tentativa = NOW(),
                    ultimo_sucesso = NOW(),
                    status = 'SUCESSO'
            """, (codigo, tipo))
        else:
            cursor.execute("""
                INSERT INTO precos_catalogo_controle 
                    (codigo_catalogo, tipo, tentativas_totais, ultima_tentativa, ultimo_erro, status)
                VALUES (%s, %s, 1, NOW(), %s, 'ERRO')
                ON CONFLICT (codigo_catalogo)
                DO UPDATE SET
                    tentativas_totais = precos_catalogo_controle.tentativas_totais + 1,
                    ultima_tentativa = NOW(),
                    ultimo_erro = %s,
                    status = 'ERRO'
            """, (codigo, tipo, error_msg, error_msg))
        
        conn.commit()
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"Erro ao atualizar controle para c√≥digo {codigo}: {e}")

# =====================================================
# FUN√á√ïES DE BUSCA DE C√ìDIGOS
# =====================================================

def get_pending_codes() -> List[Tuple[str, str]]:
    """Retorna lista de (codigo_catalogo, tipo) priorizando nunca processados"""
    try:
        if CONFIG["MODO_TESTE"]:
            logger.warning("‚ö†Ô∏è  MODO TESTE ATIVADO ‚ö†Ô∏è")
            logger.warning(f"Processando apenas c√≥digos: {CONFIG['TESTE_CODIGOS']}")
            
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'itens_compra' 
                  AND column_name IN ('coditemcatalogo', 'codigoitemcatalogo')
            """)
            
            result = cursor.fetchone()
            col_itens = result[0] if result else 'coditemcatalogo'
            
            test_codes = []
            for codigo in CONFIG["TESTE_CODIGOS"]:
                cursor.execute(f"""
                    SELECT DISTINCT LOWER(materialouserviconome)
                    FROM itens_compra
                    WHERE TRIM(TRAILING '0' FROM TRIM(TRAILING '.' FROM REGEXP_REPLACE({col_itens}, '\.0+$', ''))) = %s
                    LIMIT 1
                """, (codigo,))
                
                result = cursor.fetchone()
                if result:
                    tipo_lower = result[0]
                    tipo = 'MATERIAL' if 'material' in tipo_lower else 'SERVICO'
                    test_codes.append((codigo, tipo))
                else:
                    test_codes.append((codigo, 'MATERIAL'))
            
            cursor.close()
            conn.close()
            
            logger.info(f"Total de c√≥digos em MODO TESTE: {len(test_codes)}")
            return test_codes
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        logger.debug("üîç Identificando coluna de c√≥digo de cat√°logo...")
        cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'itens_compra' 
              AND column_name IN ('coditemcatalogo', 'codigoitemcatalogo')
        """)
        
        result_itens = cursor.fetchone()
        
        if not result_itens:
            logger.error("Nenhuma coluna de c√≥digo de cat√°logo encontrada")
            cursor.close()
            conn.close()
            return []
        
        col_itens = result_itens[0]
        logger.debug(f"‚úì Coluna identificada: {col_itens}")
        
        logger.debug("üîç Buscando c√≥digos pendentes no banco...")
        query = f"""
        WITH codigos_itens AS (
            SELECT DISTINCT 
                TRIM(TRAILING '0' FROM TRIM(TRAILING '.' FROM REGEXP_REPLACE({col_itens}, '\.0+$', ''))) as codigo,
                LOWER(materialouserviconome) as tipo_lower
            FROM itens_compra
            WHERE {col_itens} IS NOT NULL 
              AND {col_itens} != ''
              AND materialouserviconome IS NOT NULL
        )
        SELECT 
            ci.codigo,
            CASE 
                WHEN ci.tipo_lower LIKE '%material%' THEN 'MATERIAL'
                WHEN ci.tipo_lower LIKE '%servi%' THEN 'SERVICO'
                ELSE 'MATERIAL'
            END as tipo
        FROM codigos_itens ci
        LEFT JOIN precos_catalogo_controle ctrl ON ci.codigo = ctrl.codigo_catalogo
        WHERE ci.codigo ~ '^[0-9]+$'
          AND (ctrl.tentativas_totais IS NULL OR ctrl.tentativas_totais < {CONFIG['MAX_ERRORS_PER_CODE']})
        ORDER BY 
            CASE 
                WHEN ctrl.codigo_catalogo IS NULL THEN 0
                WHEN ctrl.status = 'ERRO' THEN 1
                WHEN ctrl.status = 'SUCESSO' THEN 2
                ELSE 3
            END,
            ctrl.ultima_tentativa ASC NULLS FIRST,
            ci.codigo::INT
        """
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        logger.info(f"Total de c√≥digos para processar: {len(results)}")
        return results
        
    except Exception as e:
        logger.error(f"Erro ao buscar c√≥digos pendentes: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise DatabaseError(f"Falha ao buscar c√≥digos: {e}")

# =====================================================
# FUN√á√ïES DE EXTRA√á√ÉO DA API
# =====================================================

def fetch_all_pages(codigo: str, tipo: str) -> Optional[pd.DataFrame]:
    """Busca todas as p√°ginas de um c√≥digo"""
    try:
        logger.debug(f"üåê Iniciando busca na API para c√≥digo {codigo} ({tipo})")
        
        endpoint = CONFIG["ENDPOINTS"][tipo]
        url = f"{CONFIG['PNCP_BASE_URL']}/{endpoint}"
        logger.debug(f"üìç URL: {url}")
        
        all_data = []
        pagina = 1
        
        while True:
            params = {
                'pagina': pagina,
                'codigoItemCatalogo': codigo
            }
            
            if tipo == "MATERIAL":
                params['tamanhoPagina'] = CONFIG["PAGE_SIZE"]
            
            logger.info(f"üîç Buscando c√≥digo {codigo} ({tipo}) - P√°gina {pagina}")
            logger.debug(f"üìã Par√¢metros: {params}")
            
            try:
                logger.debug("‚è≥ Fazendo requisi√ß√£o HTTP...")
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                logger.debug(f"‚úì Resposta recebida - Status: {response.status_code}")
            except (Timeout, ConnectionError) as e:
                logger.error(f"‚ùå Timeout/ConnectionError na API: {e}")
                raise APIError(f"erro ao puxar os dados da API - timeout ap√≥s 30s")
            except requests.exceptions.HTTPError as e:
                if e.response.status_code in [429, 503]:
                    logger.error(f"‚ùå API bloqueada/indispon√≠vel (HTTP {e.response.status_code})")
                    raise APIError(f"erro ao puxar os dados da API - rate limit/bloqueio ({e.response.status_code})")
                logger.error(f"‚ùå Erro HTTP na API: {e}")
                raise APIError(f"erro ao puxar os dados da API - HTTP {e.response.status_code}")
            
            logger.debug("üìù Decodificando conte√∫do CSV...")
            content = response.content.decode('utf-8-sig')
            
            if not content.strip():
                logger.debug("‚ö†Ô∏è Conte√∫do vazio - fim da pagina√ß√£o")
                break
            
            lines = content.strip().split('\n')
            logger.debug(f"üìä {len(lines)} linhas recebidas")
            
            if lines and 'totalRegistros:' in lines[-1]:
                logger.debug("üîß Removendo linha de metadados do final")
                lines = lines[:-1]
            
            if len(lines) <= 1:
                logger.debug("‚ö†Ô∏è Apenas cabe√ßalho - sem dados")
                break
            
            clean_csv = '\n'.join(lines)
            
            logger.debug("üîÑ Convertendo CSV para DataFrame...")
            df_page = pd.read_csv(
                StringIO(clean_csv),
                sep=';',
                encoding='utf-8',
                on_bad_lines='warn',
                engine='python',
                dtype=str,
                keep_default_na=False
            )
            
            if df_page.empty:
                logger.debug("‚ö†Ô∏è DataFrame vazio ap√≥s parse")
                break
            
            logger.debug(f"‚úì {len(df_page)} registros parseados nesta p√°gina")
            all_data.append(df_page)
            
            if len(df_page) < CONFIG["PAGE_SIZE"]:
                logger.debug(f"‚úì √öltima p√°gina (menos de {CONFIG['PAGE_SIZE']} registros)")
                break
            
            pagina += 1
            logger.debug("‚è≥ Aguardando 1s antes da pr√≥xima p√°gina...")
            time.sleep(1)
        
        if not all_data:
            logger.warning(f"‚ö†Ô∏è Nenhum dado encontrado para c√≥digo {codigo}")
            return None
        
        logger.debug(f"üîó Concatenando {len(all_data)} p√°ginas...")
        df_final = pd.concat(all_data, ignore_index=True)
        logger.info(f"‚úÖ Total de {len(df_final)} registros obtidos para c√≥digo {codigo}")
        
        return df_final
        
    except APIError:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro inesperado ao processar c√≥digo {codigo}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise DataValidationError(f"erro ao processar dados CSV - {str(e)[:100]}")

# =====================================================
# FUN√á√ïES DE TRANSFORMA√á√ÉO
# =====================================================

def map_csv_to_schema(df: pd.DataFrame) -> pd.DataFrame:
    """Mapeia colunas do CSV para o schema do banco"""
    if df.empty:
        return pd.DataFrame(columns=list(PRECOS_SCHEMA.keys()) + ['data_extracao', 'versao_script'])
    
    logger.debug(f"üîÑ Iniciando transforma√ß√£o dos dados...")
    logger.info(f"üìä Registros no CSV original: {len(df)}")
    logger.debug(f"üìã Total de colunas no CSV: {len(df.columns)}")
    
    # Normaliza√ß√£o
    logger.debug("üîß Normalizando nomes das colunas...")
    df.columns = [normalizar_nome_coluna(col) for col in df.columns]
    logger.debug(f"‚úì Colunas normalizadas: {df.columns.tolist()[:5]}...")
    
    # Valida√ß√£o de colunas obrigat√≥rias
    logger.debug("‚úÖ Validando colunas obrigat√≥rias...")
    if 'id_compra' not in df.columns or 'numero_item_compra' not in df.columns:
        logger.error("‚ùå Colunas obrigat√≥rias 'id_compra' e/ou 'numero_item_compra' n√£o encontradas")
        logger.error(f"Colunas dispon√≠veis: {df.columns.tolist()}")
        raise DataValidationError("erro ao validar dados CSV - colunas obrigat√≥rias ausentes")
    
    # Constru√ß√£o do idcompraitem
    logger.debug("üî® Construindo coluna idcompraitem...")
    df['idcompraitem_construido'] = (
        df['id_compra'].astype(str).str.strip() + 
        df['numero_item_compra'].astype(str).str.strip().str.replace('.0', '', regex=False).str.zfill(5)
    )
    logger.debug(f"‚úì idcompraitem constru√≠do (exemplo): {df['idcompraitem_construido'].iloc[0]}")
    
    # Tratamento de duplicatas
    registros_antes = len(df)
    logger.debug(f"üîç Verificando duplicatas (total antes: {registros_antes})...")
    
    if 'data_hora_atualizacao_item' in df.columns:
        logger.debug("üìÖ Ordenando por data de atualiza√ß√£o...")
        df['data_hora_atualizacao_item'] = pd.to_datetime(
            df['data_hora_atualizacao_item'], 
            errors='coerce'
        )
        
        df = df.sort_values('data_hora_atualizacao_item', ascending=False)
        df = df.drop_duplicates(subset=['idcompraitem_construido'], keep='first')
        
        registros_removidos = registros_antes - len(df)
        if registros_removidos > 0:
            logger.warning(f"‚ö†Ô∏è  {registros_removidos} duplicatas removidas")
    else:
        df = df.drop_duplicates(subset=['idcompraitem_construido'], keep='first')
    
    logger.info(f"üìä Registros ap√≥s deduplica√ß√£o: {len(df)}")
    
    # =====================================================
    # MAPEAMENTO COM CONVERS√ÉO CORRIGIDA
    # =====================================================
    logger.debug("üîÑ Mapeando colunas para o schema do banco...")
    
    column_mapping = {
        'idcompraitem_construido': ('idcompraitem', 'string'),
        'id_compra': ('idcompra', 'string'),
        'numero_item_compra': ('numeroitemcompra', 'integer'),  # ‚Üê CORRIGIDO
        'codigo_item_catalogo': ('coditemcatalogo', 'string'),
        'descricao_item': ('descricaodetalhada', 'string'),
        'quantidade': ('quantidadehomologada', 'decimal'),
        'sigla_unidade_medida': ('unidademedida', 'string'),
        'preco_unitario': ('valorunitariohomologado', 'decimal'),
        'percentual_maior_desconto': ('percentualdesconto', 'decimal'),
        'marca': ('marca', 'string'),
        'ni_fornecedor': ('nifornecedor', 'string'),
        'nome_fornecedor': ('nomefornecedor', 'string'),
        'codigo_uasg': ('unidadeorgaocodigounidade', 'string'),
        'nome_uasg': ('unidadeorgaonomeunidade', 'string'),
        'estado': ('unidadeorgaouf', 'string'),
        'data_compra': ('datacompra', 'date'),  # ‚Üê CORRIGIDO
    }
    
    result_data = {}
    
    for csv_col, (schema_col, col_type) in column_mapping.items():
        if csv_col in df.columns:
            if col_type == 'decimal':
                logger.debug(f"üî¢ Convertendo campo num√©rico: {csv_col} ‚Üí {schema_col}")
                result_data[schema_col] = df[csv_col].apply(convert_brazilian_number_to_decimal)
            elif col_type == 'integer':
                logger.debug(f"üî¢ Convertendo campo inteiro: {csv_col} ‚Üí {schema_col}")
                result_data[schema_col] = df[csv_col].apply(convert_to_integer_safe)
            elif col_type == 'date':
                logger.debug(f"üìÖ Convertendo campo data: {csv_col} ‚Üí {schema_col}")
                result_data[schema_col] = df[csv_col].apply(convert_to_date_safe)
            else:
                result_data[schema_col] = df[csv_col].apply(convert_to_string_safe)
            
            not_null_count = result_data[schema_col].notna().sum()
            logger.debug(f"‚úì {csv_col} ‚Üí {schema_col} ({not_null_count}/{len(df)} n√£o-nulos)")
        else:
            if schema_col != 'marca':
                logger.debug(f"‚ö†Ô∏è Coluna '{csv_col}' n√£o encontrada no CSV")
            result_data[schema_col] = [None] * len(df)
    
    # Adicionar colunas faltantes do schema
    for col in PRECOS_SCHEMA.keys():
        if col not in result_data:
            result_data[col] = [None] * len(df)
    
    result_df = pd.DataFrame(result_data)
    
    logger.debug("‚ûï Adicionando metadados...")
    result_df['data_extracao'] = datetime.utcnow()
    result_df['versao_script'] = CONFIG["SCRIPT_VERSION"]
    
    logger.info(f"‚úÖ DataFrame final: {len(result_df)} registros, {len(result_df.columns)} colunas")
    
    # Verifica√ß√£o de colunas NULL cr√≠ticas
    logger.debug("üîç Verificando colunas cr√≠ticas...")
    colunas_criticas = ['quantidadehomologada', 'unidademedida', 'valorunitariohomologado', 'numeroitemcompra']
    for col in colunas_criticas:
        null_count = result_df[col].isna().sum()
        not_null_count = result_df[col].notna().sum()
        if null_count > 0:
            logger.warning(f"‚ö†Ô∏è  Coluna '{col}': {not_null_count} preenchidos, {null_count} NULL")
    
    return result_df

# =====================================================
# FUN√á√ïES DE CARGA
# =====================================================

def load_precos_to_cockroach(df: pd.DataFrame) -> bool:
    """Carrega pre√ßos no CockroachDB"""
    if df.empty:
        logger.warning("DataFrame vazio - nada para inserir")
        return False
    
    try:
        logger.debug("üîó Conectando ao banco de dados...")
        conn = get_db_connection()
        cursor = conn.cursor()
        
        columns = list(PRECOS_SCHEMA.keys()) + ['data_extracao', 'versao_script']
        placeholders = ', '.join(['%s'] * len(columns))
        columns_str = ', '.join(columns)
        
        logger.debug("üìù Preparando query de inser√ß√£o...")
        insert_query = f"""
            INSERT INTO precos_catalogo ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT (idcompraitem)
            DO UPDATE SET 
                idcompra = EXCLUDED.idcompra,
                numeroitemcompra = EXCLUDED.numeroitemcompra,
                coditemcatalogo = EXCLUDED.coditemcatalogo,
                unidadeorgaocodigounidade = EXCLUDED.unidadeorgaocodigounidade,
                unidadeorgaonomeunidade = EXCLUDED.unidadeorgaonomeunidade,
                unidadeorgaouf = EXCLUDED.unidadeorgaouf,
                descricaodetalhada = EXCLUDED.descricaodetalhada,
                quantidadehomologada = EXCLUDED.quantidadehomologada,
                unidademedida = EXCLUDED.unidademedida,
                valorunitariohomologado = EXCLUDED.valorunitariohomologado,
                percentualdesconto = EXCLUDED.percentualdesconto,
                marca = EXCLUDED.marca,
                nifornecedor = EXCLUDED.nifornecedor,
                nomefornecedor = EXCLUDED.nomefornecedor,
                datacompra = EXCLUDED.datacompra,
                data_extracao = EXCLUDED.data_extracao,
                versao_script = EXCLUDED.versao_script
        """
        
        logger.debug("üîÑ Convertendo DataFrame para tuplas...")
        data_tuples = [tuple(row) for row in df[columns].replace({np.nan: None, pd.NaT: None}).values]
        
        logger.info(f"üíæ Inserindo {len(data_tuples)} registros na tabela precos_catalogo...")
        execute_batch(cursor, insert_query, data_tuples, page_size=1000)
        
        logger.debug("‚úÖ Commit da transa√ß√£o...")
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"‚úÖ {len(df)} registros de pre√ßos inseridos/atualizados com sucesso")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao inserir pre√ßos: {e}")
        import traceback
        logger.error(traceback.format_exc())
        if 'conn' in locals():
            conn.rollback()
            conn.close()
        raise DatabaseError(f"erro ao inserir na tabela precos_catalogo - {str(e)[:100]}")

# =====================================================
# PROCESSAMENTO DE C√ìDIGO
# =====================================================

def process_single_code(codigo: str, tipo: str) -> Tuple[bool, Optional[str], Optional[List[str]]]:
    """
    Processa um √∫nico c√≥digo de cat√°logo
    
    Returns:
        (success, error_message, idcompra_list)
    """
    try:
        logger.info(f"{'='*70}")
        logger.info(f"üîÑ PROCESSANDO C√ìDIGO: {codigo} ({tipo})")
        logger.info(f"{'='*70}")
        
        logger.debug(f"üìç Etapa 1/3: Extra√ß√£o da API")
        try:
            df_raw = fetch_all_pages(codigo, tipo)
        except APIError as e:
            logger.error(f"‚ùå Erro na API: {e}")
            return (False, str(e), None)
        
        if df_raw is None or df_raw.empty:
            logger.warning(f"‚ö†Ô∏è Sem dados para c√≥digo {codigo}")
            return (False, 'nenhum dado retornado pela API', None)
        
        # Extrai lista de idcompra antes de transformar
        logger.debug("üìã Normalizando colunas temporariamente para extrair idcompra...")
        df_temp = df_raw.copy()
        df_temp.columns = [normalizar_nome_coluna(col) for col in df_temp.columns]
        
        if 'id_compra' in df_temp.columns:
            idcompra_list = df_temp['id_compra'].unique().tolist()
            logger.debug(f"‚úì {len(idcompra_list)} idcompra √∫nicos encontrados")
        else:
            idcompra_list = []
            logger.warning("‚ö†Ô∏è Coluna id_compra n√£o encontrada")
        
        logger.debug(f"üìç Etapa 2/3: Transforma√ß√£o dos dados")
        try:
            df_clean = map_csv_to_schema(df_raw)
        except DataValidationError as e:
            logger.error(f"‚ùå Erro na valida√ß√£o: {e}")
            return (False, str(e), idcompra_list)
        
        logger.debug(f"üìç Etapa 3/3: Carga no banco de dados")
        try:
            load_precos_to_cockroach(df_clean)
            logger.info(f"‚úÖ C√≥digo {codigo} processado com sucesso!")
            return (True, None, idcompra_list)
        except DatabaseError as e:
            logger.error(f"‚ùå Erro no banco: {e}")
            return (False, str(e), idcompra_list)
        
    except Exception as e:
        logger.error(f"‚ùå Erro inesperado ao processar c√≥digo {codigo}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return (False, f'erro inesperado - {str(e)[:100]}', None)

def process_code_with_retry(codigo: str, tipo: str) -> bool:
    """Processa c√≥digo com retry para erros de API e atualiza Sheets"""
    logger.debug(f"üöÄ Iniciando processamento de {codigo} com retry habilitado")
    
    success, error_msg, idcompra_list = process_single_code(codigo, tipo)
    
    if success:
        logger.debug("‚úÖ Sucesso na primeira tentativa")
        status = f"sucesso - {len(idcompra_list) if idcompra_list else 0} registros inseridos"
        update_control_record(codigo, tipo, True)
        write_catalogo_status(codigo, idcompra_list, status)
        return True
    
    # Verifica se √© erro de API para fazer retry
    if error_msg and ('erro ao puxar os dados da API' in error_msg or 'timeout' in error_msg or 'rate limit' in error_msg):
        logger.warning(f"‚ö†Ô∏è  Erro de API detectado para {codigo}, aguardando {CONFIG['API_ERROR_RETRY_DELAY']}s para retry...")
        time.sleep(CONFIG['API_ERROR_RETRY_DELAY'])
        
        logger.info(f"üîÑ Tentativa 2/2 para c√≥digo {codigo}")
        success, error_msg, idcompra_list = process_single_code(codigo, tipo)
        
        if success:
            logger.info("‚úÖ Sucesso na segunda tentativa")
            status = f"sucesso - {len(idcompra_list) if idcompra_list else 0} registros inseridos"
            update_control_record(codigo, tipo, True)
            write_catalogo_status(codigo, idcompra_list, status)
            return True
    
    # Falha definitiva
    logger.error(f"‚ùå Falha definitiva para c√≥digo {codigo}: {error_msg}")
    
    # Registra erro de banco para log especial
    if error_msg and 'erro ao inserir na tabela' in error_msg:
        global db_errors_log
        db_errors_log.append({
            'codigo': codigo,
            'tipo': tipo,
            'erro': error_msg,
            'timestamp': datetime.now()
        })
        logger.error(f"üóÑÔ∏è Erro de banco registrado para {codigo}")
    
    update_control_record(codigo, tipo, False, error_msg)
    write_catalogo_status(codigo, idcompra_list, error_msg)
    
    return False

# =====================================================
# PROCESSAMENTO PARALELO
# =====================================================

def process_batch_parallel(batch: List[Tuple[str, str]]) -> Tuple[int, int, int]:
    """Processa lote de c√≥digos em paralelo com stagger"""
    logger.debug(f"üîÄ Iniciando processamento paralelo de {len(batch)} c√≥digos")
    
    sucessos = 0
    falhas_api = 0
    falhas_outras = 0
    
    with ThreadPoolExecutor(max_workers=CONFIG["PARALLEL_REQUESTS"]) as executor:
        futures = {}
        
        for i, (codigo, tipo) in enumerate(batch):
            if i > 0:
                logger.debug(f"‚è≥ Stagger delay: {CONFIG['STAGGER_DELAY_SECONDS']}s")
                time.sleep(CONFIG["STAGGER_DELAY_SECONDS"])
            
            future = executor.submit(process_code_with_retry, codigo, tipo)
            futures[future] = (codigo, tipo)
            logger.info(f"üöÄ Thread iniciada para c√≥digo: {codigo}")
        
        for future in as_completed(futures):
            codigo, tipo = futures[future]
            
            try:
                success = future.result()
                
                if success:
                    sucessos += 1
                    logger.info(f"‚úÖ Conclu√≠do com sucesso: {codigo}")
                else:
                    # Verifica tipo de erro no controle
                    try:
                        conn = get_db_connection()
                        cursor = conn.cursor()
                        cursor.execute(
                            "SELECT ultimo_erro FROM precos_catalogo_controle WHERE codigo_catalogo = %s",
                            (codigo,)
                        )
                        result = cursor.fetchone()
                        cursor.close()
                        conn.close()
                        
                        if result and result[0] and 'erro ao puxar os dados da API' in result[0]:
                            falhas_api += 1
                        else:
                            falhas_outras += 1
                    except:
                        falhas_outras += 1
                    
                    logger.error(f"‚ùå Falhou: {codigo}")
                    
            except Exception as e:
                logger.error(f"‚ùå Exce√ß√£o n√£o capturada para {codigo}: {e}")
                falhas_outras += 1
    
    logger.debug(f"‚úì Lote conclu√≠do: {sucessos} sucessos, {falhas_api} falhas API, {falhas_outras} outras falhas")
    return (sucessos, falhas_api, falhas_outras)

# =====================================================
# FUN√á√ÉO PRINCIPAL
# =====================================================

def main():
    """Orquestra√ß√£o principal do pipeline de pre√ßos"""
    global execution_start_time, should_stop, db_errors_log
    
    execution_start_time = datetime.now()
    should_stop = False
    db_errors_log = []
    
    logger.info("="*80)
    if CONFIG["MODO_TESTE"]:
        logger.info("‚ö†Ô∏è  EXECUTANDO EM MODO TESTE ‚ö†Ô∏è")
        logger.info(f"C√≥digos de teste: {CONFIG['TESTE_CODIGOS']}")
    else:
        logger.info("=== Pipeline de Pre√ßos de Cat√°logo (PRODU√á√ÉO) ===")
    logger.info(f"Vers√£o do script: {CONFIG['SCRIPT_VERSION']}")
    logger.info(f"Tempo limite: {CONFIG['EXECUTION_TIME_LIMIT_HOURS']} hora(s)")
    logger.info(f"Processamento paralelo: {CONFIG['PARALLEL_REQUESTS']} requisi√ß√µes simult√¢neas")
    logger.info("="*80)
    
    try:
        logger.info("üìù Etapa 1/5: Cria√ß√£o de tabelas de controle")
        create_control_table()
        
        logger.info("üìù Etapa 2/5: Inicializa√ß√£o da planilha Google Sheets")
        initialize_sheets_tab()
        
        logger.info("üìù Etapa 3/5: Obten√ß√£o de c√≥digos pendentes")
        pending_codes = get_pending_codes()
        
        if not pending_codes:
            logger.info("‚ÑπÔ∏è Nenhum c√≥digo pendente para processar")
            return
        
        logger.info("üìù Etapa 4/5: Populando c√≥digos na planilha")
        populate_initial_codes()
        
        logger.info("üìù Etapa 5/5: Processamento dos c√≥digos")
        logger.info(f"üìä Total de c√≥digos a processar: {len(pending_codes)}")
        
        total = len(pending_codes)
        processed = 0
        total_success = 0
        total_failed = 0
        consecutive_api_errors = 0
        
        for i in range(0, len(pending_codes), CONFIG["PARALLEL_REQUESTS"]):
            if not check_execution_time():
                logger.warning("‚è∞ Encerrando execu√ß√£o por limite de tempo")
                break
            
            batch = pending_codes[i:i + CONFIG["PARALLEL_REQUESTS"]]
            batch_num = (i // CONFIG["PARALLEL_REQUESTS"]) + 1
            total_batches = (len(pending_codes) + CONFIG["PARALLEL_REQUESTS"] - 1) // CONFIG["PARALLEL_REQUESTS"]
            
            logger.info(f"\n{'='*80}")
            logger.info(f">>> LOTE {batch_num}/{total_batches}")
            logger.info(f"C√≥digos neste lote: {[c for c, _ in batch]}")
            logger.info(f"Progresso total: {processed}/{total} ({(processed/total*100):.1f}%)")
            logger.info(f"Erros API consecutivos: {consecutive_api_errors}/{CONFIG['MAX_CONSECUTIVE_API_ERRORS']}")
            logger.info(f"{'='*80}")
            
            sucessos, falhas_api, falhas_outras = process_batch_parallel(batch)
            
            processed += len(batch)
            total_success += sucessos
            total_failed += (falhas_api + falhas_outras)
            
            if falhas_api > 0:
                consecutive_api_errors += falhas_api
            else:
                consecutive_api_errors = 0
            
            logger.info(f"\nüìä Resumo do Lote {batch_num}:")
            logger.info(f"  ‚úÖ Sucessos: {sucessos}")
            logger.info(f"  ‚ùå Falhas API: {falhas_api}")
            logger.info(f"  ‚ùå Outras falhas: {falhas_outras}")
            
            if consecutive_api_errors >= CONFIG["MAX_CONSECUTIVE_API_ERRORS"]:
                logger.critical(f"\nüõë LIMITE DE ERROS DE API ATINGIDO ({CONFIG['MAX_CONSECUTIVE_API_ERRORS']})")
                logger.critical("Poss√≠vel problema sist√™mico com a API - encerrando execu√ß√£o")
                break
            
            # Delay entre lotes
            if i + CONFIG["PARALLEL_REQUESTS"] < len(pending_codes):
                logger.debug(f"‚è≥ Aguardando 2s antes do pr√≥ximo lote...")
                time.sleep(2)
        
        # Relat√≥rio final
        elapsed_time = datetime.now() - execution_start_time
        
        logger.info("\n" + "="*80)
        logger.info("=== EXECU√á√ÉO CONCLU√çDA ===")
        logger.info(f"Tempo de execu√ß√£o: {elapsed_time}")
        logger.info(f"Total processado: {processed}/{total} ({(processed/total*100):.1f}%)")
        logger.info(f"‚úÖ Sucessos: {total_success} ({(total_success/processed*100):.1f}%)")
        logger.info(f"‚ùå Falhas: {total_failed} ({(total_failed/processed*100):.1f}%)")
        
        if db_errors_log:
            logger.warning(f"\n‚ö†Ô∏è  ERROS DE BANCO DE DADOS ({len(db_errors_log)}):")
            for err in db_errors_log:
                logger.warning(f"  - {err['codigo']} ({err['tipo']}): {err['erro'][:100]}")
        
        logger.info("="*80)
        
    except DatabaseError as e:
        logger.critical(f"‚ùå ERRO CR√çTICO DE BANCO: {e}")
        logger.critical("Imposs√≠vel continuar - verifique conex√£o e credenciais")
        raise
        
    except Exception as e:
        logger.error(f"‚ùå Erro fatal n√£o tratado: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()

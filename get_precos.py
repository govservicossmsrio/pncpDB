import os
import time
import logging
import requests
import pandas as pd
import numpy as np
from datetime import datetime
from io import StringIO
import psycopg2
from psycopg2.extras import execute_batch
from typing import Dict, List, Optional, Tuple

# =====================================================
# CONFIGURA√á√ÉO
# =====================================================

CONFIG = {
    "COCKROACH_CONNECTION_STRING": os.getenv(
        "COCKROACH_CONNECTION_STRING",
        "postgresql://sgc_admin:<password>@scary-quetzal-18026.j77.aws-us-east-1.cockroachlabs.cloud:26257/defaultdb?sslmode=require"
    ),
    "PNCP_BASE_URL": "https://dadosabertos.compras.gov.br",
    "ENDPOINTS": {
        "MATERIAL": "modulo-pesquisa-preco/1.1_consultarMaterial_CSV",
        "SERVICO": "modulo-pesquisa-preco/3.1_consultarServico_CSV"
    },
    "PAGE_SIZE": 200,
    "BATCH_SIZE": 10,
    "SUCCESS_DELAY_SECONDS": 2,
    "MAX_RETRIES_PER_ITEM": 3,
    "RETRY_DELAY_SECONDS": 5,
    "MAX_CONSECUTIVE_FAILURES": 30,
}

# =====================================================
# LOGGING
# =====================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =====================================================
# SCHEMA PRECOS_CATALOGO (MIN√öSCULAS)
# =====================================================

PRECOS_SCHEMA = {
    "idcompra": "STRING",
    "iditemcompra": "STRING",
    "forma": "STRING",
    "modalidade": "STRING",
    "criteriojulgamento": "STRING",
    "numeroitemcompra": "INTEGER",
    "descricaoitem": "STRING",
    "codigoitemcatalogo": "STRING",
    "nomeunidademedida": "STRING",
    "siglaunidademedida": "STRING",
    "nomeunidadefornecimento": "STRING",
    "siglaunidadefornecimento": "STRING",
    "capacidadeunidadefornecimento": "FLOAT",
    "quantidade": "FLOAT",
    "precounitario": "FLOAT",
    "percentualmaiordesconto": "FLOAT",
    "nifornecedor": "STRING",
    "nomefornecedor": "STRING",
    "marca": "STRING",
    "codigouasg": "STRING",
    "nomeuasg": "STRING",
    "codigomunicipio": "STRING",
    "municipio": "STRING",
    "estado": "STRING",
    "codigoorgao": "STRING",
    "nomeorgao": "STRING",
    "poder": "STRING",
    "esfera": "STRING",
    "datacompra": "DATE",
    "datahoraatualizacaocompra": "TIMESTAMP",
    "datahoraatualizacaoitem": "TIMESTAMP",
    "dataresultado": "DATE",
    "datahoraatualizacaouasg": "TIMESTAMP",
    "codigoclasse": "STRING",
    "nomeclasse": "STRING",
}

# =====================================================
# CONEX√ÉO COM COCKROACHDB
# =====================================================

def get_db_connection():
    """Cria conex√£o com CockroachDB"""
    try:
        conn = psycopg2.connect(CONFIG["COCKROACH_CONNECTION_STRING"])
        return conn
    except Exception as e:
        logger.error(f"Erro ao conectar com CockroachDB: {e}")
        raise

# =====================================================
# FUN√á√ïES DE BUSCA DE C√ìDIGOS
# =====================================================

def get_pending_codes() -> List[Tuple[str, str]]:
    """
    Retorna lista de (codigoitemcatalogo, tipo) priorizando:
    1. C√≥digos nunca buscados
    2. C√≥digos mais antigos (data_extracao ASC)
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = """
        WITH codigos_itens AS (
            SELECT DISTINCT 
                codigoitemcatalogo,
                LOWER(materialouserviconome) as tipo_lower
            FROM itens_compra
            WHERE codigoitemcatalogo IS NOT NULL 
              AND codigoitemcatalogo != ''
              AND materialouserviconome IS NOT NULL
        ),
        codigos_processados AS (
            SELECT DISTINCT 
                codigoitemcatalogo,
                MAX(data_extracao) as ultima_extracao
            FROM precos_catalogo
            GROUP BY codigoitemcatalogo
        )
        SELECT 
            ci.codigoitemcatalogo,
            CASE 
                WHEN ci.tipo_lower LIKE '%material%' THEN 'MATERIAL'
                WHEN ci.tipo_lower LIKE '%servi%' THEN 'SERVICO'
                ELSE 'MATERIAL'
            END as tipo
        FROM codigos_itens ci
        LEFT JOIN codigos_processados cp ON ci.codigoitemcatalogo = cp.codigoitemcatalogo
        ORDER BY 
            CASE WHEN cp.codigoitemcatalogo IS NULL THEN 0 ELSE 1 END,
            cp.ultima_extracao ASC NULLS FIRST,
            ci.codigoitemcatalogo
        """
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        logger.info(f"Total de c√≥digos para processar: {len(results)}")
        return results
        
    except Exception as e:
        logger.error(f"Erro ao buscar c√≥digos pendentes: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return []

# =====================================================
# FUN√á√ïES DE EXTRA√á√ÉO DA API
# =====================================================

def fetch_all_pages(codigo: str, tipo: str) -> Optional[pd.DataFrame]:
    """
    Busca todas as p√°ginas de um c√≥digo na API correspondente
    Retorna DataFrame consolidado ou None
    """
    try:
        endpoint = CONFIG["ENDPOINTS"][tipo]
        url = f"{CONFIG['PNCP_BASE_URL']}/{endpoint}"
        
        all_data = []
        pagina = 1
        
        while True:
            params = {
                'pagina': pagina,
                'codigoItemCatalogo': codigo
            }
            
            # Adicionar tamanhoPagina apenas para Material
            if tipo == "MATERIAL":
                params['tamanhoPagina'] = CONFIG["PAGE_SIZE"]
            
            logger.info(f"Buscando c√≥digo {codigo} ({tipo}) - P√°gina {pagina}")
            
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            # Decodificar com UTF-8 para caracteres especiais
            content = response.content.decode('utf-8-sig')
            
            if not content.strip():
                logger.warning(f"Resposta vazia para c√≥digo {codigo} na p√°gina {pagina}")
                break
            
            # Remover √∫ltima linha de metadados (totalRegistros: ...)
            lines = content.strip().split('\n')
            if lines and 'totalRegistros:' in lines[-1]:
                lines = lines[:-1]
            
            if len(lines) <= 1:  # Apenas header ou vazio
                logger.info(f"Sem mais dados na p√°gina {pagina}")
                break
            
            clean_csv = '\n'.join(lines)
            
            # Parse CSV
            df_page = pd.read_csv(StringIO(clean_csv), sep='\t', encoding='utf-8')
            
            if df_page.empty:
                logger.info(f"DataFrame vazio na p√°gina {pagina}")
                break
            
            all_data.append(df_page)
            
            # Verificar se h√° pr√≥xima p√°gina (inferir do tamanho)
            if len(df_page) < CONFIG["PAGE_SIZE"]:
                logger.info(f"√öltima p√°gina alcan√ßada (registros: {len(df_page)})")
                break
            
            pagina += 1
            time.sleep(1)  # Rate limiting entre p√°ginas
        
        if not all_data:
            logger.warning(f"Nenhum dado encontrado para c√≥digo {codigo}")
            return None
        
        # Consolidar todas as p√°ginas
        df_final = pd.concat(all_data, ignore_index=True)
        logger.info(f"‚úì Total de {len(df_final)} registros para c√≥digo {codigo}")
        
        return df_final
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Erro HTTP ao buscar c√≥digo {codigo}: {e}")
        return None
    except Exception as e:
        logger.error(f"Erro ao processar c√≥digo {codigo}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

# =====================================================
# FUN√á√ïES DE TRANSFORMA√á√ÉO
# =====================================================

def convert_column_type(series: pd.Series, target_type: str) -> pd.Series:
    """Converte tipos de dados"""
    try:
        if target_type == "STRING":
            return series.astype(str).replace(['nan', 'None', '<NA>', ''], None)
        elif target_type == "INTEGER":
            return pd.to_numeric(series, errors='coerce').astype('Int64')
        elif target_type == "FLOAT":
            return pd.to_numeric(series, errors='coerce')
        elif target_type == "DATE":
            return pd.to_datetime(series, errors='coerce').dt.date
        elif target_type == "TIMESTAMP":
            return pd.to_datetime(series, errors='coerce', utc=True)
        else:
            return series
    except Exception as e:
        logger.warning(f"Erro ao converter coluna: {e}")
        return series

def normalize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """Normaliza nomes de colunas para min√∫sculas"""
    df.columns = df.columns.str.lower().str.strip()
    return df

def map_and_clean_dataframe(df: pd.DataFrame, schema: Dict[str, str]) -> pd.DataFrame:
    """Mapeia e limpa DataFrame conforme schema"""
    if df.empty:
        return pd.DataFrame(columns=list(schema.keys()) + ['data_extracao'])
    
    # Normalizar nomes de colunas
    df = normalize_column_names(df)
    
    result_df = pd.DataFrame()
    
    # Mapear colunas do schema
    for col, dtype in schema.items():
        if col in df.columns:
            result_df[col] = convert_column_type(df[col], dtype)
        else:
            result_df[col] = None
            logger.debug(f"Coluna {col} n√£o encontrada no CSV")
    
    result_df['data_extracao'] = datetime.utcnow()
    
    return result_df

# =====================================================
# FUN√á√ïES DE CARGA
# =====================================================

def load_precos_to_cockroach(df: pd.DataFrame) -> bool:
    """Carrega pre√ßos no CockroachDB"""
    if df.empty:
        logger.warning("DataFrame vazio - nada para inserir")
        return False
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        columns = list(PRECOS_SCHEMA.keys()) + ['data_extracao']
        placeholders = ', '.join(['%s'] * len(columns))
        columns_str = ', '.join(columns)
        
        # Usar iditemcompra como chave √∫nica
        insert_query = f"""
            INSERT INTO precos_catalogo ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT (idcompraitem)
            DO UPDATE SET data_extracao = EXCLUDED.data_extracao
        """
        
        # Preparar dados
        data_tuples = [tuple(row) for row in df[columns].replace({np.nan: None}).values]
        
        # Executar batch insert
        execute_batch(cursor, insert_query, data_tuples, page_size=1000)
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"‚úì {len(df)} registros de pre√ßos inseridos/atualizados")
        return True
        
    except Exception as e:
        logger.error(f"Erro ao inserir pre√ßos: {e}")
        import traceback
        logger.error(traceback.format_exc())
        if 'conn' in locals():
            conn.rollback()
            conn.close()
        return False

# =====================================================
# PROCESSAMENTO DE C√ìDIGO
# =====================================================

def process_single_code(codigo: str, tipo: str) -> bool:
    """Processa um √∫nico c√≥digo de cat√°logo"""
    try:
        logger.info(f"--- Processando c√≥digo: {codigo} ({tipo}) ---")
        
        # Buscar dados da API
        df_raw = fetch_all_pages(codigo, tipo)
        
        if df_raw is None or df_raw.empty:
            logger.warning(f"Sem dados para c√≥digo {codigo}")
            return False
        
        # Transformar dados
        df_clean = map_and_clean_dataframe(df_raw, PRECOS_SCHEMA)
        
        # Carregar no banco
        success = load_precos_to_cockroach(df_clean)
        
        return success
        
    except Exception as e:
        logger.error(f"Erro ao processar c√≥digo {codigo}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

# =====================================================
# FUN√á√ÉO PRINCIPAL
# =====================================================

def main():
    """Orquestra√ß√£o principal do pipeline de pre√ßos"""
    logger.info("=== Iniciando Pipeline de Pre√ßos de Cat√°logo ===")
    
    try:
        # 1. Buscar c√≥digos pendentes
        pending_codes = get_pending_codes()
        
        if not pending_codes:
            logger.info("Nenhum c√≥digo pendente para processar")
            return
        
        # 2. Processar em batches com controle de falhas consecutivas
        total = len(pending_codes)
        processed = 0
        failed = 0
        consecutive_failures = 0
        
        for i in range(0, len(pending_codes), CONFIG["BATCH_SIZE"]):
            batch = pending_codes[i:i + CONFIG["BATCH_SIZE"]]
            
            logger.info(f"\n>>> Processando lote {i//CONFIG['BATCH_SIZE'] + 1} ({len(batch)} c√≥digos)")
            logger.info(f"Falhas consecutivas: {consecutive_failures}/{CONFIG['MAX_CONSECUTIVE_FAILURES']}")
            
            for codigo, tipo in batch:
                # Verificar limite de falhas consecutivas
                if consecutive_failures >= CONFIG["MAX_CONSECUTIVE_FAILURES"]:
                    logger.critical(f"üõë LIMITE DE FALHAS CONSECUTIVAS ATINGIDO ({CONFIG['MAX_CONSECUTIVE_FAILURES']})")
                    logger.critical("Parando execu√ß√£o para evitar bloqueio de IP")
                    logger.info(f"\nResumo at√© parada:")
                    logger.info(f"  - Processados: {processed}/{total}")
                    logger.info(f"  - Falhas: {failed}")
                    logger.info(f"  - Restantes: {total - processed - failed}")
                    return
                
                retry_count = 0
                success = False
                
                while retry_count < CONFIG["MAX_RETRIES_PER_ITEM"] and not success:
                    if retry_count > 0:
                        logger.info(f"Tentativa {retry_count + 1} para c√≥digo {codigo}")
                        time.sleep(CONFIG["RETRY_DELAY_SECONDS"])
                    
                    success = process_single_code(codigo, tipo)
                    retry_count += 1
                
                if success:
                    processed += 1
                    consecutive_failures = 0
                    logger.info(f"‚úÖ Sucesso | Falhas consecutivas resetadas para 0")
                    time.sleep(CONFIG["SUCCESS_DELAY_SECONDS"])
                else:
                    failed += 1
                    consecutive_failures += 1
                    logger.error(f"‚ùå C√≥digo {codigo} falhou ap√≥s {CONFIG['MAX_RETRIES_PER_ITEM']} tentativas")
                    logger.warning(f"‚ö†Ô∏è  Falhas consecutivas: {consecutive_failures}/{CONFIG['MAX_CONSECUTIVE_FAILURES']}")
            
        
        # 3. Relat√≥rio final
        logger.info("\n=== Pipeline Conclu√≠do ===")
        logger.info(f"Total: {total} c√≥digos")
        logger.info(f"Sucesso: {processed}")
        logger.info(f"Falhas: {failed}")
        logger.info(f"Taxa de sucesso: {(processed/total)*100:.2f}%")
        
    except Exception as e:
        logger.error(f"Erro no pipeline principal: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()

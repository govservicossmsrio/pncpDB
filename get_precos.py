import os
import time
import logging
import requests
import pandas as pd
import numpy as np
from datetime import datetime
from io import StringIO
import psycopg2
from psycopg2.extras import execute_batch
from typing import Dict, List, Optional, Tuple
import re

# =====================================================
# CONFIGURA√á√ÉO
# =====================================================

CONFIG = {
    "COCKROACH_CONNECTION_STRING": os.getenv(
        "COCKROACH_CONNECTION_STRING",
        "postgresql://sgc_admin:<password>@scary-quetzal-18026.j77.aws-us-east-1.cockroachlabs.cloud:26257/defaultdb?sslmode=require"
    ),
    "PNCP_BASE_URL": "https://dadosabertos.compras.gov.br",
    "ENDPOINTS": {
        "MATERIAL": "modulo-pesquisa-preco/1.1_consultarMaterial_CSV",
        "SERVICO": "modulo-pesquisa-preco/3.1_consultarServico_CSV"
    },
    "PAGE_SIZE": 200,
    "BATCH_SIZE": 10,
    "SUCCESS_DELAY_SECONDS": 2,
    "MAX_RETRIES_PER_ITEM": 3,
    "RETRY_DELAY_SECONDS": 5,
    "MAX_CONSECUTIVE_FAILURES": 30,
    "SCRIPT_VERSION": "v1.1.0",
    
    # ===== MODO TESTE =====
    "MODO_TESTE": False,
    "TESTE_CODIGOS": ["439495"],
}

# =====================================================
# LOGGING
# =====================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =====================================================
# SCHEMA PRECOS_CATALOGO
# =====================================================

PRECOS_SCHEMA = {
    "idcompraitem": "STRING", 
    "idcompra": "STRING",
    "numeroitemcompra": "INTEGER",  
    "coditemcatalogo": "STRING",
    "unidadeorgaocodigounidade": "STRING",
    "unidadeorgaonomeunidade": "STRING",
    "unidadeorgaouf": "STRING",
    "descricaodetalhada": "STRING",
    "quantidadehomologada": "FLOAT",
    "unidademedida": "STRING",
    "valorunitariohomologado": "FLOAT",
    "percentualdesconto": "FLOAT",
    "marca": "STRING",
    "nifornecedor": "STRING",
    "nomefornecedor": "STRING",
    "datacompra": "DATE",
    "datahoraatualizacaoitem": "TIMESTAMP",
}

# =====================================================
# FUN√á√ïES AUXILIARES
# =====================================================

def normalizar_nome_coluna(nome: str) -> str:
    """Converte CamelCase para snake_case"""
    if not isinstance(nome, str):
        return ''
    s = nome.strip()
    # camelCase ‚Üí snake_case: idCompra ‚Üí id_compra
    s = re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', s)
    # Remove caracteres especiais
    s = re.sub(r'[^a-zA-Z0-9_]+', '_', s)
    return s.lower().strip('_')

# =====================================================
# CONEX√ÉO COM COCKROACHDB
# =====================================================

def get_db_connection():
    """Cria conex√£o com CockroachDB"""
    try:
        conn = psycopg2.connect(CONFIG["COCKROACH_CONNECTION_STRING"])
        return conn
    except Exception as e:
        logger.error(f"Erro ao conectar com CockroachDB: {e}")
        raise

# =====================================================
# FUN√á√ïES DE BUSCA DE C√ìDIGOS
# =====================================================

def get_pending_codes() -> List[Tuple[str, str]]:
    """Retorna lista de (codigo_catalogo, tipo)"""
    try:
        if CONFIG["MODO_TESTE"]:
            logger.warning("‚ö†Ô∏è  MODO TESTE ATIVADO ‚ö†Ô∏è")
            logger.warning(f"Processando apenas c√≥digos: {CONFIG['TESTE_CODIGOS']}")
            
            conn = get_db_connection()
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = 'itens_compra' 
                  AND column_name IN ('coditemcatalogo', 'codigoitemcatalogo')
            """)
            
            result = cursor.fetchone()
            col_itens = result[0] if result else 'coditemcatalogo'
            
            test_codes = []
            for codigo in CONFIG["TESTE_CODIGOS"]:
                cursor.execute(f"""
                    SELECT DISTINCT LOWER(materialouserviconome)
                    FROM itens_compra
                    WHERE TRIM(TRAILING '0' FROM TRIM(TRAILING '.' FROM REGEXP_REPLACE({col_itens}, '\.0+$', ''))) = %s
                    LIMIT 1
                """, (codigo,))
                
                result = cursor.fetchone()
                if result:
                    tipo_lower = result[0]
                    tipo = 'MATERIAL' if 'material' in tipo_lower else 'SERVICO'
                    test_codes.append((codigo, tipo))
                else:
                    test_codes.append((codigo, 'MATERIAL'))
            
            cursor.close()
            conn.close()
            
            logger.info(f"Total de c√≥digos em MODO TESTE: {len(test_codes)}")
            return test_codes
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'itens_compra' 
              AND column_name IN ('coditemcatalogo', 'codigoitemcatalogo')
        """)
        
        result_itens = cursor.fetchone()
        
        if not result_itens:
            logger.error("Nenhuma coluna de c√≥digo de cat√°logo encontrada")
            cursor.close()
            conn.close()
            return []
        
        col_itens = result_itens[0]
        
        query = f"""
        WITH codigos_itens AS (
            SELECT DISTINCT 
                TRIM(TRAILING '0' FROM TRIM(TRAILING '.' FROM REGEXP_REPLACE({col_itens}, '\.0+$', ''))) as codigo,
                LOWER(materialouserviconome) as tipo_lower
            FROM itens_compra
            WHERE {col_itens} IS NOT NULL 
              AND {col_itens} != ''
              AND materialouserviconome IS NOT NULL
        ),
        codigos_processados AS (
            SELECT DISTINCT 
                TRIM(TRAILING '0' FROM TRIM(TRAILING '.' FROM REGEXP_REPLACE(coditemcatalogo, '\.0+$', ''))) as codigo,
                MAX(data_extracao) as ultima_extracao
            FROM precos_catalogo
            GROUP BY TRIM(TRAILING '0' FROM TRIM(TRAILING '.' FROM REGEXP_REPLACE(coditemcatalogo, '\.0+$', '')))
        )
        SELECT 
            ci.codigo,
            CASE 
                WHEN ci.tipo_lower LIKE '%material%' THEN 'MATERIAL'
                WHEN ci.tipo_lower LIKE '%servi%' THEN 'SERVICO'
                ELSE 'MATERIAL'
            END as tipo
        FROM codigos_itens ci
        LEFT JOIN codigos_processados cp ON ci.codigo = cp.codigo
        WHERE ci.codigo ~ '^[0-9]+$'
        ORDER BY 
            CASE WHEN cp.codigo IS NULL THEN 0 ELSE 1 END,
            cp.ultima_extracao ASC NULLS FIRST,
            ci.codigo::INT
        """
        
        cursor.execute(query)
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        logger.info(f"Total de c√≥digos para processar: {len(results)}")
        return results
        
    except Exception as e:
        logger.error(f"Erro ao buscar c√≥digos pendentes: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return []

# =====================================================
# FUN√á√ïES DE EXTRA√á√ÉO DA API
# =====================================================

def fetch_all_pages(codigo: str, tipo: str) -> Optional[pd.DataFrame]:
    """Busca todas as p√°ginas de um c√≥digo"""
    try:
        endpoint = CONFIG["ENDPOINTS"][tipo]
        url = f"{CONFIG['PNCP_BASE_URL']}/{endpoint}"
        
        all_data = []
        pagina = 1
        
        while True:
            params = {
                'pagina': pagina,
                'codigoItemCatalogo': codigo
            }
            
            if tipo == "MATERIAL":
                params['tamanhoPagina'] = CONFIG["PAGE_SIZE"]
            
            logger.info(f"Buscando c√≥digo {codigo} ({tipo}) - P√°gina {pagina}")
            
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            content = response.content.decode('utf-8-sig')
            
            if not content.strip():
                break
            
            lines = content.strip().split('\n')
            if lines and 'totalRegistros:' in lines[-1]:
                lines = lines[:-1]
            
            if len(lines) <= 1:
                break
            
            clean_csv = '\n'.join(lines)
            
            df_page = pd.read_csv(
                StringIO(clean_csv),
                sep=';',
                encoding='utf-8',
                on_bad_lines='warn',
                engine='python',
                dtype=str
            )
            
            if df_page.empty:
                break
            
            all_data.append(df_page)
            
            if len(df_page) < CONFIG["PAGE_SIZE"]:
                break
            
            pagina += 1
            time.sleep(1)
        
        if not all_data:
            logger.warning(f"Nenhum dado encontrado para c√≥digo {codigo}")
            return None
        
        df_final = pd.concat(all_data, ignore_index=True)
        logger.info(f"‚úì Total de {len(df_final)} registros para c√≥digo {codigo}")
        
        return df_final
        
    except Exception as e:
        logger.error(f"Erro ao processar c√≥digo {codigo}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None

# =====================================================
# FUN√á√ïES DE TRANSFORMA√á√ÉO
# =====================================================

def convert_column_type(series: pd.Series, target_type: str) -> pd.Series:
    """Converte tipos de dados"""
    try:
        if target_type == "STRING":
            result = series.astype(str).replace(['nan', 'None', '<NA>', ''], None)
            if result is not None and hasattr(result, 'str'):
                result = result.str.replace(r'\.0+$', '', regex=True)
            return result
        elif target_type == "INTEGER":
            return pd.to_numeric(series, errors='coerce').astype('Int64')
        elif target_type == "FLOAT":
            return pd.to_numeric(series, errors='coerce')
        elif target_type == "DATE":
            return pd.to_datetime(series, errors='coerce').dt.date
        elif target_type == "TIMESTAMP":
            return pd.to_datetime(series, errors='coerce')
        else:
            return series
    except Exception as e:
        logger.warning(f"Erro ao converter coluna: {e}")
        return series

def map_csv_to_schema(df: pd.DataFrame) -> pd.DataFrame:
    """Mapeia colunas do CSV para o schema do banco"""
    if df.empty:
        return pd.DataFrame(columns=list(PRECOS_SCHEMA.keys()) + ['data_extracao', 'versao_script'])
    
    logger.info(f"Registros no CSV original: {len(df)}")
    logger.info(f"Colunas originais (primeiras 10): {df.columns.tolist()[:10]}")
    
    # Normaliza√ß√£o robusta
    df.columns = [normalizar_nome_coluna(col) for col in df.columns]
    df = df.where(pd.notna(df), None)
    
    logger.info(f"Colunas normalizadas (primeiras 10): {df.columns.tolist()[:10]}")
    
    # =====================================================
    # CONSTRU√á√ÉO DO idcompraitem (PRIMARY KEY COMPOSTA)
    # =====================================================
    
    if 'id_compra' not in df.columns or 'numero_item_compra' not in df.columns:
        logger.error("‚ùå Colunas obrigat√≥rias 'id_compra' e/ou 'numero_item_compra' n√£o encontradas")
        logger.error(f"Colunas dispon√≠veis: {df.columns.tolist()}")
        raise ValueError("Colunas obrigat√≥rias ausentes no CSV")
    
    # Construir idcompraitem = id_compra + numero_item_compra (5 d√≠gitos)
    df['idcompraitem_construido'] = (
        df['id_compra'].astype(str) + 
        df['numero_item_compra'].astype(str).str.zfill(5)
    )
    
    logger.info(f"‚úì idcompraitem constru√≠do (exemplo): {df['idcompraitem_construido'].iloc[0]}")
    
    # =====================================================
    # TRATAMENTO DE DUPLICATAS
    # =====================================================
    
    registros_antes = len(df)
    
    if 'data_hora_atualizacao_item' in df.columns:
        # Converter para datetime
        df['data_hora_atualizacao_item'] = pd.to_datetime(
            df['data_hora_atualizacao_item'], 
            errors='coerce'
        )
        
        # Ordenar por data (mais recente primeiro) e remover duplicatas
        df = df.sort_values('data_hora_atualizacao_item', ascending=False)
        df = df.drop_duplicates(subset=['idcompraitem_construido'], keep='first')
        
        registros_removidos = registros_antes - len(df)
        if registros_removidos > 0:
            logger.warning(f"‚ö†Ô∏è  {registros_removidos} duplicatas removidas (mantido registro mais recente)")
    else:
        logger.warning("‚ö†Ô∏è  Coluna 'data_hora_atualizacao_item' n√£o encontrada - duplicatas n√£o tratadas")
        df = df.drop_duplicates(subset=['idcompraitem_construido'], keep='first')
    
    logger.info(f"Registros ap√≥s deduplica√ß√£o: {len(df)}")
    
    # =====================================================
    # MAPEAMENTO: CSV normalizado ‚Üí Banco
    # =====================================================
    
    column_mapping = {
        # PRIMARY KEY (constru√≠da, n√£o do CSV)
        'idcompraitem_construido': 'idcompraitem',
        
        # Identificadores
        'id_compra': 'idcompra',
        'numero_item_compra': 'numeroitemcompra',
        
        # Item
        'codigo_item_catalogo': 'coditemcatalogo',
        'descricao_item': 'descricaodetalhada',
        
        # Quantidade e valores
        'quantidade': 'quantidadehomologada',
        'sigla_unidade_medida': 'unidademedida',
        'nome_unidade_medida': 'unidademedida',  # fallback
        'preco_unitario': 'valorunitariohomologado',
        'percentual_maior_desconto': 'percentualdesconto',
        
        # Fornecedor
        'marca': 'marca',
        'ni_fornecedor': 'nifornecedor',
        'nome_fornecedor': 'nomefornecedor',
        
        # √ìrg√£o
        'codigo_uasg': 'unidadeorgaocodigounidade',
        'nome_uasg': 'unidadeorgaonomeunidade',
        'estado': 'unidadeorgaouf',
        
        # Datas
        'data_compra': 'datacompra',
        'data_hora_atualizacao_item': 'datahoraatualizacaoitem',
    }
    
    # Construir dicion√°rio de dados
    result_data = {}
    
    for csv_col, schema_col in column_mapping.items():
        if csv_col in df.columns:
            # Evitar sobrescrever se j√° mapeado (prioridade para primeira ocorr√™ncia)
            if schema_col not in result_data or result_data[schema_col] is None:
                result_data[schema_col] = convert_column_type(
                    df[csv_col],
                    PRECOS_SCHEMA.get(schema_col, "STRING")
                )
                logger.debug(f"‚úì Mapeado: {csv_col} ‚Üí {schema_col}")
        else:
            if schema_col not in result_data:
                logger.debug(f"‚ö†Ô∏è  Coluna '{csv_col}' n√£o encontrada no CSV")
    
    # Adicionar colunas faltantes do schema (com None)
    for col in PRECOS_SCHEMA.keys():
        if col not in result_data:
            result_data[col] = [None] * len(df)
            logger.debug(f"‚ö†Ô∏è  Coluna '{col}' preenchida com NULL (n√£o encontrada no CSV)")
    
    result_df = pd.DataFrame(result_data)
    
    result_df['data_extracao'] = datetime.utcnow()
    result_df['versao_script'] = CONFIG["SCRIPT_VERSION"]
    
    logger.info(f"‚úì DataFrame final: {len(result_df)} registros, {len(result_df.columns)} colunas")
    
    # Verifica√ß√£o de colunas NULL cr√≠ticas
    colunas_criticas = ['quantidadehomologada', 'unidademedida', 'valorunitariohomologado']
    for col in colunas_criticas:
        null_count = result_df[col].isna().sum()
        if null_count > 0:
            logger.warning(f"‚ö†Ô∏è  Coluna '{col}' tem {null_count} valores NULL de {len(result_df)} registros")
    
    return result_df

# =====================================================
# FUN√á√ïES DE CARGA
# =====================================================

def load_precos_to_cockroach(df: pd.DataFrame) -> bool:
    """Carrega pre√ßos no CockroachDB"""
    if df.empty:
        logger.warning("DataFrame vazio - nada para inserir")
        return False
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        columns = list(PRECOS_SCHEMA.keys()) + ['data_extracao', 'versao_script']
        placeholders = ', '.join(['%s'] * len(columns))
        columns_str = ', '.join(columns)
        
        # PRIMARY KEY √© idcompraitem
        insert_query = f"""
            INSERT INTO precos_catalogo ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT (idcompraitem)
            DO UPDATE SET 
                idcompra = EXCLUDED.idcompra,
                numeroitemcompra = EXCLUDED.numeroitemcompra,
                coditemcatalogo = EXCLUDED.coditemcatalogo,
                unidadeorgaocodigounidade = EXCLUDED.unidadeorgaocodigounidade,
                unidadeorgaonomeunidade = EXCLUDED.unidadeorgaonomeunidade,
                unidadeorgaouf = EXCLUDED.unidadeorgaouf,
                descricaodetalhada = EXCLUDED.descricaodetalhada,
                quantidadehomologada = EXCLUDED.quantidadehomologada,
                unidademedida = EXCLUDED.unidademedida,
                valorunitariohomologado = EXCLUDED.valorunitariohomologado,
                percentualdesconto = EXCLUDED.percentualdesconto,
                marca = EXCLUDED.marca,
                nifornecedor = EXCLUDED.nifornecedor,
                nomefornecedor = EXCLUDED.nomefornecedor,
                datacompra = EXCLUDED.datacompra,
                datahoraatualizacaoitem = EXCLUDED.datahoraatualizacaoitem,
                data_extracao = EXCLUDED.data_extracao,
                versao_script = EXCLUDED.versao_script
        """
        
        data_tuples = [tuple(row) for row in df[columns].replace({np.nan: None}).values]
        
        logger.info(f"Inserindo {len(data_tuples)} registros...")
        execute_batch(cursor, insert_query, data_tuples, page_size=1000)
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"‚úì {len(df)} registros de pre√ßos inseridos/atualizados")
        return True
        
    except Exception as e:
        logger.error(f"Erro ao inserir pre√ßos: {e}")
        import traceback
        logger.error(traceback.format_exc())
        if 'conn' in locals():
            conn.rollback()
            conn.close()
        return False

# =====================================================
# PROCESSAMENTO DE C√ìDIGO
# =====================================================

def process_single_code(codigo: str, tipo: str) -> bool:
    """Processa um √∫nico c√≥digo de cat√°logo"""
    try:
        logger.info(f"--- Processando c√≥digo: {codigo} ({tipo}) ---")
        
        df_raw = fetch_all_pages(codigo, tipo)
        
        if df_raw is None or df_raw.empty:
            logger.warning(f"Sem dados para c√≥digo {codigo}")
            return False
        
        df_clean = map_csv_to_schema(df_raw)
        
        success = load_precos_to_cockroach(df_clean)
        
        return success
        
    except Exception as e:
        logger.error(f"Erro ao processar c√≥digo {codigo}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

# =====================================================
# FUN√á√ÉO PRINCIPAL
# =====================================================

def main():
    """Orquestra√ß√£o principal do pipeline de pre√ßos"""
    
    if CONFIG["MODO_TESTE"]:
        logger.info("="*60)
        logger.info("‚ö†Ô∏è  EXECUTANDO EM MODO TESTE ‚ö†Ô∏è")
        logger.info(f"C√≥digos: {CONFIG['TESTE_CODIGOS']}")
        logger.info("="*60)
    else:
        logger.info("=== Pipeline de Pre√ßos de Cat√°logo (PRODU√á√ÉO) ===")
    
    try:
        pending_codes = get_pending_codes()
        
        if not pending_codes:
            logger.info("Nenhum c√≥digo pendente")
            return
        
        total = len(pending_codes)
        processed = 0
        failed = 0
        consecutive_failures = 0
        
        for i in range(0, len(pending_codes), CONFIG["BATCH_SIZE"]):
            batch = pending_codes[i:i + CONFIG["BATCH_SIZE"]]
            
            logger.info(f"\n>>> Lote {i//CONFIG['BATCH_SIZE'] + 1} ({len(batch)} c√≥digos)")
            logger.info(f"Falhas consecutivas: {consecutive_failures}/{CONFIG['MAX_CONSECUTIVE_FAILURES']}")
            
            for codigo, tipo in batch:
                if consecutive_failures >= CONFIG["MAX_CONSECUTIVE_FAILURES"]:
                    logger.critical(f"üõë LIMITE ATINGIDO ({CONFIG['MAX_CONSECUTIVE_FAILURES']})")
                    return
                
                retry_count = 0
                success = False
                
                while retry_count < CONFIG["MAX_RETRIES_PER_ITEM"] and not success:
                    if retry_count > 0:
                        time.sleep(CONFIG["RETRY_DELAY_SECONDS"])
                    
                    success = process_single_code(codigo, tipo)
                    retry_count += 1
                
                if success:
                    processed += 1
                    consecutive_failures = 0
                    logger.info(f"‚úÖ Sucesso")
                    time.sleep(CONFIG["SUCCESS_DELAY_SECONDS"])
                else:
                    failed += 1
                    consecutive_failures += 1
                    logger.error(f"‚ùå Falhou ap√≥s {CONFIG['MAX_RETRIES_PER_ITEM']} tentativas")
            
            logger.info(f"Progresso: {processed}/{total}")
        
        logger.info("\n=== CONCLU√çDO ===")
        
    except Exception as e:
        logger.error(f"Erro fatal: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()
